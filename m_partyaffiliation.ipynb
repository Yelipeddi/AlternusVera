{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "count of TF-IDF stemmed features\n",
      "Stemmed - 9120\n",
      "-----------------------------------\n",
      "count of TF-IDF lemmatized features\n",
      "Lemmatized - 11350\n",
      "-----------------------------------\n",
      "TF-IDF tokens\n",
      "['safer', 'safest', 'safety', 'safetyeducation', 'said', 'saida', 'saidin', 'saidpresident', 'saidsonia', 'saidthe', 'sailed', 'saint', 'sakineh', 'sakonnet', 'sal', 'salaried', 'salary', 'sale', 'salem', 'salesman', 'salestax', 'salmon', 'salmonella', 'salt', 'saltwater', 'saluted', 'salvador', 'sam', 'sameday', 'samesex', 'samevalue', 'san', 'sanction', 'sanctioned', 'sanctity', 'sanctuary', 'sand', 'sander', 'sandra', 'sandwich', 'sandy', 'sanford', 'sanger', 'sanitary', 'sanitation', 'santa', 'santarsiero', 'santorum', 'sarah', 'sarswas', 'sat', 'satellite', 'satisfaction', 'satisfied', 'saturday', 'sauced', 'saudi', 'sauk', 'savannah', 'save', 'saved', 'saving', 'savior', 'saw', 'say', 'saying', 'sayingi', 'saypresident', 'sayreville', 'saysabout', 'saysbarack', 'saysbernie', 'sayscongressman', 'saysdonald', 'saysgary', 'saysgop', 'saysgov', 'sayshillary', 'saysjeb', 'saysjosh', 'saysloretta', 'saysmichael', 'saysmichele', 'sayspatrick', 'sayspeter', 'sayspresident', 'saysproposal', 'saysrand', 'saysruben', 'sayssenate', 'saysted', 'saysthe', 'saysthere', 'saystheres', 'saystom', 'saysvirginia', 'sb', 'sboe', 'sc', 'scale', 'scalia', 'scam', 'scamming', 'scan', 'scandal', 'scanner', 'scare', 'scary', 'scenario', 'schaufler', 'schedule', 'scheduled', 'scheduling', 'scheme', 'schemer', 'schiano', 'schimel', 'schip', 'schneiders', 'scholar', 'scholarly', 'scholarship', 'school', 'schoolage', 'schoolaid', 'schoolchildren', 'schooling', 'schrader', 'schultz', 'schumer', 'schwarzenegger', 'science', 'scientific', 'scientifically', 'scientist', 'scientologists', 'scientology', 'scion', 'scoop', 'scooter', 'scope', 'score', 'scoreboard', 'scored', 'scoring', 'scott', 'scottwalker', 'scout', 'scramble', 'scrap', 'scraping', 'scratched', 'screamed', 'screaming', 'screen', 'screening', 'screeningand', 'screwdriver', 'screwed', 'scrutiny', 'scuba', 'sculpture', 'scuttle', 'sea', 'seafood', 'seal', 'sealed', 'sealevel', 'seam', 'sean', 'search', 'season', 'seat', 'seattle', 'seatwarmer', 'seaworld', 'sebelius', 'secede', 'seceding', 'second', 'secondary', 'secondbest', 'secondgeneration', 'secondgeorge', 'secondguess', 'secondhighest', 'secondlargest', 'secondlowest', 'secondmost', 'secondterm', 'secret', 'secretary', 'secretly', 'section', 'sector', 'secure', 'secured', 'secures', 'securitiesproduct', 'security', 'see', 'seed', 'seedy', 'seeing', 'seek', 'seeking', 'seem', 'seemingly', 'seems', 'seen', 'segment', 'segregated', 'seize', 'seized', 'seizing', 'seizure', 'selected', 'selecting', 'selection', 'selective', 'selectively', 'self', 'selfavowed', 'selfdefense', 'selfdestruct', 'selfemployment', 'selffinancing', 'selffunded', 'selffunding', 'selfpronounced', 'sell', 'seller', 'selling', 'sellwood', 'selma', 'semblance', 'semiautomatic', 'seminar', 'seminole', 'sen', 'senate', 'senatebecause', 'senator', 'send', 'sending', 'sends', 'seneca', 'senelect', 'senior', 'seniority', 'senmark', 'sense', 'senseless', 'sensible', 'sent', 'sentence', 'sentenced', 'sentencing', 'separate', 'sept', 'september', 'septic', 'sequester', 'sequestration', 'serf', 'serial', 'series', 'serious', 'seriously', 'sermon', 'serum', 'serve', 'served', 'server', 'service', 'serviceman', 'serving', 'session', 'sestak', 'set', 'setback', 'seton', 'setting', 'settle', 'settled', 'settlement', 'seven', 'sevenfigure', 'seventh', 'seventy', 'seventyfive', 'seventyfour', 'seventytwo', 'several', 'severance', 'severe', 'severed', 'severely', 'severity', 'sewage', 'sewer', 'sewerage', 'sex', 'sexlinked', 'sexselection', 'sexselective', 'sexting', 'sexual', 'sexually', 'sgt', 'shackle', 'shade', 'shadow', 'shady', 'shah', 'shaheen', 'shalala', 'shale', 'shall', 'shanghai', 'shape', 'share', 'shared', 'sharia', 'shariah', 'sharing', 'shark', 'sharp', 'sharpefirst', 'sharron', 'shave', 'sheaporter', 'shedding', 'sheet', 'sheikh', 'sheila', 'shelby', 'sheldon', 'shelf', 'shell', 'shelled', 'shellfish', 'shelly', 'shelter', 'sheltered', 'sheriff', 'sherrod', 'shes', 'shield', 'shift', 'shifted', 'shin', 'ship', 'shipbuilder', 'shipment', 'shipping', 'shirley', 'shirt', 'shocked', 'shocking', 'shoe', 'shoot', 'shooter', 'shootershould', 'shooting', 'shop', 'shopping', 'shore', 'short', 'shortage', 'shortcoming', 'shortest', 'shortfall', 'shortterm', 'shot', 'shouldnt', 'shoved', 'show', 'showed', 'shower', 'showered', 'showhillary', 'showing', 'shown', 'shrank', 'shred', 'shredit', 'shrink', 'shrinking', 'shrinkwrapped', 'shrub', 'shrunk', 'shunned', 'shut', 'shutdown', 'shuts', 'shutting', 'shuttle', 'sic', 'sick', 'side', 'sided', 'sideline', 'sidestep', 'sidewalk', 'sidney', 'siemens', 'sierra', 'sign', 'signature', 'signed', 'signer', 'significance', 'significant', 'significantly', 'signing', 'silberkraus', 'silence', 'silent', 'silver', 'simac', 'similar', 'similarity', 'simple', 'simply', 'simpson', 'simpsonbowles', 'simulation', 'simultaneously', 'sin', 'since', 'sincebarack', 'sincedebunked', 'sing', 'singapore', 'singing', 'single', 'singleanswer', 'singlebiggest', 'singleoccupancy', 'singleparent', 'singlepayer', 'singleyear', 'sink', 'siphon', 'sister', 'sit', 'site', 'sitin', 'sits', 'sitting', 'sittons', 'situation', 'six', 'sixdistrict', 'sixfigure', 'sixinten', 'sixpack', 'sixteen', 'sixth', 'sixthfastest', 'sixthhighest', 'sixty', 'sixtyfive', 'sixtyone', 'sixtypercent', 'sixtytwo', 'sixyear', 'size', 'sjr', 'skew', 'skill', 'skilled', 'skin', 'skip', 'skipped', 'skipping', 'skull', 'sky', 'skyline', 'skyrocket', 'skyrocketed', 'skyrocketing', 'slam', 'slapped', 'slash', 'slashed', 'slashing', 'slated', 'slaughtered', 'slave', 'slaveholder', 'slavery', 'slavesfreedom', 'sleep', 'slept', 'slice', 'slightly', 'slim', 'slime', 'slogan', 'slot', 'slouch', 'slovenia', 'slow', 'slowdown', 'slowed', 'slower', 'slowest', 'slowly', 'slush', 'sly', 'small', 'smallbusiness', 'smaller', 'smallest', 'smallminded', 'smallpox', 'smart', 'smartest', 'smartphones', 'smear', 'smell', 'smelter', 'smiley', 'smith', 'smitherman', 'smog', 'smoked', 'smoker', 'smoking', 'smuggled', 'snack', 'snail', 'sneak', 'sneaky', 'sneeze', 'sniper', 'snow', 'snowden', 'snowmobile', 'snowstorm', 'soar', 'soared', 'soaring', 'socalled', 'soccer', 'social', 'socialism', 'socialist', 'socialized', 'socially', 'society', 'socioeconomic', 'sock', 'soda', 'soetoro', 'sofa', 'soft', 'softening', 'soglins', 'soil', 'solar', 'solarcity', 'solarheated', 'sold', 'soldier', 'solely', 'solicitation', 'solicitor', 'solid', 'solitary', 'solo', 'solution', 'solv', 'solve', 'solved', 'solvency', 'solvent', 'solving', 'solyndra', 'solyndrahad', 'somali', 'somalia', 'somebody', 'somehow', 'someone', 'someplace', 'something', 'sometime', 'sometimes', 'somewhere', 'son', 'song', 'sonia', 'sonogram', 'soon', 'sooner', 'soonerthan', 'sore', 'soros', 'sorry', 'sort', 'sotomayor', 'sotomayors', 'sought', 'soul', 'sound', 'soundly', 'soup', 'source', 'south', 'southeast', 'southerland', 'southern', 'southerner', 'southwest', 'southwestern', 'sovereign', 'sovereignty', 'soviet', 'sovietstyle', 'sox', 'soy', 'spa', 'space', 'spaceshuttle', 'spain', 'span', 'spanish', 'spanishlanguage', 'spanker', 'spare', 'spared', 'spark', 'spasm', 'spawned', 'spay', 'speak', 'speaker', 'speakerdesignate', 'speakership', 'speaking', 'speaks', 'special', 'specialinterest', 'specialist', 'specializes', 'specie', 'specific', 'specifically', 'specified', 'specifying', 'specter', 'speculative', 'speculator', 'speech', 'speed', 'speedy', 'spell', 'spencer', 'spend', 'spending', 'spends', 'spent', 'spew', 'spewing', 'spike', 'spill', 'spilled', 'spillover', 'spine', 'spinnett', 'spiral', 'spiraled', 'spiraling', 'spirit', 'spiritual', 'spitting', 'split', 'splost', 'spoiler', 'spoke', 'spoken', 'spokesman', 'spokesperson', 'sponsor', 'sponsored', 'sponsoring', 'spontaneous', 'sport', 'sporting', 'spot', 'spotted', 'spotting', 'spotty', 'spouse', 'sprawl', 'sprawling', 'spray', 'spread', 'spreading', 'spree', 'spring', 'springsteen', 'sprinkler', 'spurned', 'spy', 'spying', 'squad', 'square', 'squat', 'squeezing', 'squirrel', 'ssdi', 'ssi', 'st', 'stab', 'stabbing', 'stability', 'stabilization', 'stabilize', 'stabilized', 'stabilizing', 'stable', 'stack', 'stadium', 'staff', 'staffer', 'staffing', 'stafford', 'stage', 'staged', 'staggering', 'stagnant', 'staircase', 'stake', 'stalin', 'stalking', 'stamp', 'stan', 'stance', 'stand', 'standalone', 'standard', 'standardized', 'standdown', 'standing', 'standpoint', 'stanford', 'stanley', 'staple', 'star', 'starbucks', 'stark', 'starkey', 'starr', 'start', 'started', 'startedthe', 'startedwaronpoverty', 'starting', 'startup', 'starved', 'starving', 'state', 'statebased', 'stated', 'statehired', 'statehouse', 'statemandated', 'statement', 'stateowned', 'stater', 'staterun', 'statesupported', 'stateswere', 'statewide', 'stating', 'station', 'stationed', 'statistic', 'statistical', 'statistically', 'stats', 'status', 'statute', 'staunch', 'staunchly', 'stay', 'stayathome', 'stayed', 'staying', 'steadily', 'steady', 'steakhouse', 'steam', 'steel', 'steelers', 'steelworker', 'steer', 'stefani', 'steinem', 'stellar', 'stem', 'step', 'stephanopoulos', 'stephen', 'sterilization', 'sterling', 'stern', 'steroid', 'steve', 'steven', 'stevens', 'stevie', 'stewart', 'steyer', 'sticker', 'sticking', 'stiff', 'still', 'stilltooracist', 'stimulate', 'stimulus', 'stirred', 'stock', 'stockman', 'stockpile', 'stockpiling', 'stockton', 'stokes', 'stole', 'stolen', 'stomach', 'stone', 'stoned', 'stoning', 'stood', 'stop', 'stopandfrisk', 'stopgap', 'stopped', 'stopping', 'store', 'storing', 'storm', 'story', 'stove', 'straight', 'straightforward', 'strain', 'strama', 'stranded', 'strangerdanger', 'strangle', 'strangulation', 'strategic', 'strategist', 'strategy', 'stratosphere', 'straus', 'strauss', 'strawberry', 'strayed', 'stream', 'streaming', 'street', 'streetcar', 'streetsign', 'strength', 'strengthen', 'strengthened', 'stress', 'stretch', 'strickland', 'strict', 'strictest', 'strictly', 'stride', 'strife', 'strike', 'striking', 'strikingly', 'stringent', 'strip', 'stripclub', 'stripping', 'stroke', 'strong', 'stronger', 'strongest', 'strongly', 'struck', 'structural', 'structurally', 'structure', 'struggle', 'struggling', 'stuart', 'stuck', 'student', 'studentathletes', 'studio', 'study', 'studying', 'stuff', 'stuffed', 'stupid', 'sturtevant', 'stuxnet', 'style', 'stylist', 'styrofoam', 'subcommittee', 'subhuman', 'subject', 'subjected', 'subjective', 'subjugation', 'submarine', 'subminimum', 'submit', 'submitted', 'subpar', 'subpoena', 'subpoenaed', 'subprime', 'subsaharan', 'subscriber', 'subscription', 'subsequent', 'subsidiary', 'subsidize', 'subsidized', 'subsidy', 'substance', 'substantial', 'substantially', 'substitute', 'substitution', 'suburb', 'subway', 'succeed', 'succeeded', 'succeeding', 'succeeds', 'success', 'successful', 'successfully', 'successor', 'sucked', 'suction', 'sudafed', 'sudden', 'suddenly', 'sue', 'sued', 'suffer', 'suffered', 'suffering', 'sufficient', 'sugar', 'suggest', 'suggested', 'suggesting', 'suggests', 'suicide', 'suing', 'suit', 'suite', 'sulfide', 'sullivan', 'sum', 'summer', 'summit', 'sun', 'sunday', 'sunny', 'sunset', 'sunshine', 'sunstein', 'sununu', 'sununus', 'super', 'superdelegate', 'superdelegates', 'superfund', 'superintendent', 'superior', 'superiority', 'supermax', 'superpacs', 'superpower', 'superrich', 'superstorm', 'supervised', 'supervisor', 'supplement', 'supplemental', 'supplier', 'supply', 'supplysider', 'support', 'supported', 'supportedamnesty', 'supporter', 'supporting', 'supportive', 'supposed', 'supposedly', 'suppression', 'supremacist', 'supreme', 'surcharge', 'sure', 'surface', 'surge', 'surged', 'surgeon', 'surgery', 'surgical', 'surpass', 'surpassed', 'surplus', 'surprised', 'surrender', 'surrogate', 'surround', 'surrounded', 'surrounding', 'surtax', 'surveil', 'surveillance', 'survey', 'survival', 'survive', 'survived', 'surviving', 'survivor', 'susan', 'suspect', 'suspected', 'suspend', 'suspended', 'suspending', 'suspicion', 'suspicious', 'sutton', 'suv', 'suzanne', 'swap', 'swapped', 'swarming', 'swastika', 'swat', 'swaziland', 'swearingin', 'sweater', 'sweden', 'sweeping', 'sweepstakes', 'sweetheart', 'swift', 'swim', 'swimming', 'swindled', 'swine', 'swing', 'swiss', 'switch', 'switched', 'switching', 'switzerland', 'sworn', 'sylvia', 'symbol', 'sympathetic', 'sympathized', 'sympathy', 'synagogue', 'syndicate', 'syndrome', 'synthetic', 'syria', 'syrian', 'system', 'systematic', 'systematically', 'tab', 'table', 'tackle', 'tactic', 'taft', 'tag', 'tail', 'tailgating', 'tailpipe', 'tainted', 'taj', 'take', 'taken', 'takeover', 'taker', 'taking', 'takingbob', 'talal', 'taliban', 'talk', 'talked', 'talking', 'tall', 'tallahassee', 'taller', 'tallied', 'tally', 'tammy', 'tampa', 'tamperproof', 'tank', 'tanked', 'tanker', 'tanking', 'tanning', 'tap', 'tape', 'tapped', 'tapping', 'tar', 'target', 'targeted', 'targeting', 'tariff', 'tarkanian', 'tarp', 'tarpon', 'task', 'tattoo', 'taught', 'taveras', 'tax', 'taxation', 'taxcredit', 'taxcut', 'taxed', 'taxexempt', 'taxfriendly', 'taxi', 'taxing', 'taxman', 'taxpayer', 'taxpayerfunded', 'taxpayerowned', 'taxpayerpaid', 'taxpayerpaidfor', 'taxpayersubsidized', 'taxwas', 'taylor', 'tb', 'tea', 'teach', 'teacher', 'teaching', 'teachout', 'team', 'teamed', 'teamster', 'teapot', 'tear', 'tearing', 'tech', 'technical', 'technically', 'technique', 'technology', 'ted', 'teddy', 'teen', 'teenage', 'teenaged', 'teenager', 'tehran', 'teilhet', 'telecast', 'telecom', 'telecommunication', 'telemarketers', 'television', 'tell', 'teller', 'telling', 'temperature', 'temporarily', 'temporary', 'ten', 'tenandahalf', 'tenant', 'tend', 'tended', 'tends', 'tenet', 'tennessean', 'tennessee', 'tenpoint', 'tent', 'tenth', 'tenure', 'tenured', 'term', 'terminal', 'terminally', 'terminated', 'terrell', 'terrible', 'terrificand', 'territorial', 'territory', 'terror', 'terrorism', 'terrorist', 'terroristsfriends', 'terry', 'terrygorman', 'test', 'tested', 'tester', 'testified', 'testify', 'testimony', 'testing', 'testprep', 'texan', 'texas', 'texashad', 'texasmexico', 'text', 'textbook', 'texting', 'textured', 'tf', 'thanfind', 'thank', 'thanked', 'thanks', 'thanksgiving', 'thankyous', 'thanthe', 'thatcaptured', 'thatcher', 'thatd', 'thats', 'thatthe', 'thawed', 'thc', 'the1992', 'theaffordable', 'theatrically', 'thecbo', 'thedford', 'thedollar', 'thefirst', 'theft', 'thehousebudget', 'theinsurance', 'theislamic', 'themto', 'thencongressman', 'thengeorgia', 'thengov', 'thenpresident', 'thensecretary', 'thensen', 'theocracy', 'theorist', 'theory', 'therapy', 'there', 'thereabout', 'thereafter', 'thereby', 'therefore', 'therell', 'thesis', 'theus', 'thewater', 'theyd', 'theyll', 'theyre', 'theyve', 'thibaut', 'thin', 'thing', 'think', 'third', 'thirdbiggest', 'thirdgrade', 'thirdgraders', 'thirdhighest', 'thirdlargest', 'thirdlowest', 'thirdtrimester', 'thirdworld', 'thirteen', 'thirty', 'thirtyeight', 'thirtyfive', 'thirtyfour', 'thirtyone', 'thom', 'thomas', 'thompson', 'thoroughly', 'though', 'thought', 'thousand', 'thousandfold', 'threat', 'threaten', 'threatened', 'threatening', 'threatens', 'three', 'threeblack', 'threefifths', 'threefold', 'threefourths', 'threequarters', 'threestate', 'threestory', 'threeyearolds', 'threshold', 'threw', 'thrive', 'throat', 'throughout', 'throw', 'throwing', 'thrown', 'thug', 'thumbprint', 'thurbert', 'thurmond', 'thus', 'thwart', 'thwarted', 'ticket', 'ticketing', 'ticking', 'tide', 'tie', 'tied', 'tier', 'tighten', 'tightened', 'till', 'tiller', 'tillis', 'tim', 'timber', 'timberlake', 'time', 'timeline', 'timetable', 'timing', 'tina', 'tiny', 'tip', 'tipping', 'tire', 'tireless', 'title', 'titled', 'to1', 'tobacco', 'today', 'todd', 'toddler', 'toetotoe', 'tofrom', 'together', 'tohave', 'toilet', 'toimplement', 'told', 'toldevery', 'toledo', 'toll', 'tollfree', 'tom', 'tomahawk', 'tommy', 'tomorrow', 'ton', 'tone', 'tonight', 'took', 'tool', 'toomey', 'toothache', 'toothbrush', 'toothpaste', 'top', 'top25', 'topdown', 'topfive', 'topic', 'topperforming', 'toprated', 'topsecret', 'topselling', 'toraise', 'tore', 'toreport', 'tort', 'torture', 'torturing', 'tossup', 'total', 'totaled', 'totaling', 'totally', 'touch', 'touched', 'tough', 'toughen', 'tougher', 'toughest', 'tour', 'touring', 'tourism', 'tourist', 'tout', 'touted', 'touting', 'tovo', 'tow', 'toward', 'towards', 'tower', 'town', 'townsend', 'towpath', 'toxic', 'toy', 'trace', 'traced', 'track', 'tracked', 'tracker', 'tractor', 'trade', 'tradedependent', 'trademark', 'trading', 'tradition', 'traditional', 'traditionally', 'traffic', 'trafficked', 'trafficking', 'tragedy', 'tragic', 'tragically', 'trail', 'trailed', 'trailing', 'train', 'trained', 'training', 'trait', 'transaction', 'transcript', 'transexual', 'transfer', 'transferred', 'transgender', 'transgression', 'transit', 'transition', 'translate', 'translates', 'translator', 'transmissible', 'transmission', 'transmitted', 'transpacific', 'transparency', 'transparent', 'transport', 'transportation', 'transporting', 'transtexas', 'transvaginal', 'trap', 'trapped', 'trapping', 'trash', 'traumatic', 'travel', 'traveled', 'traveler', 'traveling', 'travesty', 'travis', 'trayvon', 'treason', 'treasurer', 'treasury', 'treat', 'treated', 'treating', 'treatment', 'treaty', 'treatyinfringes', 'tree', 'trek', 'tremendous', 'trend', 'trendline', 'trenton', 'trespass', 'trespassing', 'treuhaft', 'trey', 'trial', 'tribe', 'tribute', 'trick', 'trickledown', 'tried', 'trig', 'trigger', 'triggering', 'trillion', 'trilliondollar', 'trimet', 'trimets', 'trimming', 'trip', 'triple', 'triplebunked', 'tripled', 'tripling', 'tripoli', 'trippi', 'triumph', 'trolley', 'troop', 'trooper', 'trouble', 'troubling', 'truancy', 'truck', 'trucker', 'true', 'truethat', 'truly', 'truman', 'trump', 'trumpet', 'trumphas', 'trunk', 'trust', 'trusted', 'trustworthy', 'truth', 'truthful', 'try', 'trying', 'tsa', 'tshirt', 'tuberculosis', 'tucson', 'tuesday', 'tuggey', 'tuition', 'tuitionfree', 'tune', 'tuneups', 'turbine', 'turkey', 'turn', 'turnaround', 'turned', 'turner', 'turning', 'turnout', 'turnpike', 'turtle', 'tv', 'tva', 'tweet', 'tweeted', 'tweeting', 'twelve', 'twenty', 'twentyfive', 'twentyfour', 'twentyone', 'twentysixth', 'twentythree', 'twentytwo', 'twice', 'twinkling', 'twitter', 'two', 'twoandahalf', 'twobedroom', 'twoday', 'twodecade', 'twodozennuclear', 'twostate', 'twothirds', 'twotour', 'twoyear', 'tx', 'txdems', 'tying', 'tyler', 'type', 'typical', 'typically', 'tyson', 'u', 'u201cthirtyfour', 'uaw', 'uber', 'udall', 'udalls', 'ufo', 'ugly', 'uhaul', 'uk', 'ukraine', 'ukrainian', 'uline', 'ultimately', 'ultrasound', 'umatilla', 'umpqua', 'un', 'unable', 'unacceptable', 'unaccompanied', 'unaccountable', 'unaccounted', 'unamerican', 'unanimous', 'unanimously', 'unarmed', 'unauthorized', 'unbeknownst', 'unbelievable', 'unborn', 'uncertainty', 'unchanged', 'unchecked', 'unclaimed', 'uncle', 'uncollected', 'uncompensated', 'unconstitutional', 'unconstitutionally', 'undefeated', 'undeniable', 'underage', 'undercover', 'undercurrent', 'undereducated', 'underemployed', 'underfunded', 'underfunding', 'undergo', 'undergraduate', 'underground', 'undermine', 'underpaid', 'underperformed', 'underperforming', 'underprivileged', 'underqualified', 'understand', 'understanding', 'understands', 'understood', 'undertaxed', 'underused', 'underutilized', 'underwater', 'underwear', 'undisclosed', 'undocumented', 'unelected', 'unemployed', 'unemployment', 'unequal', 'unequivocal', 'unequivocally', 'unethical', 'unexpected', 'unfair', 'unfairness', 'unfortunately', 'unfriendly', 'unfunded', 'unger', 'unhealthy', 'unheard', 'uniform', 'unilateral', 'unilaterally', 'unincorporated', 'uninsurable', 'uninsured', 'unintended', 'uninterrupted', 'union', 'unionization', 'unionized', 'unionoperated', 'unique', 'unit', 'united', 'universal', 'universe', 'university', 'universityhave', 'univision', 'unkind', 'unknown', 'unlawful', 'unleashes', 'unless', 'unlicensed', 'unlike', 'unlimited', 'unloading', 'unlocking', 'unmatched', 'unmitigated', 'unnecessarily', 'unnecessary', 'unopposed', 'unpaid', 'unpopular', 'unprecedented', 'unreasonable', 'unrecognized', 'unregulated', 'unrelated', 'unrest', 'unsafe', 'unsealed', 'unseemly', 'unsigned', 'unstable', 'unsuccessful', 'unsuccessfully', 'unsustainable', 'untagged', 'unthinkable', 'untold', 'untoward', 'untrained', 'untreated', 'untrustworthy', 'untruthful', 'unturned', 'unused', 'unusual', 'unveiled', 'unwanted', 'unwinding', 'upcoming', 'update', 'updated', 'upgrade', 'upgraded', 'upgrading', 'upheld', 'uphold', 'uploaded', 'upon', 'upper', 'ups', 'upscale', 'upset', 'upstate', 'upstream', 'uptick', 'upwards', 'uranium', 'urban', 'urged', 'urgent', 'urging', 'urgingcongress', 'urinal', 'us', 'usa', 'usage', 'usborn', 'usda', 'use', 'used', 'usefulness', 'user', 'ushered', 'using', 'usisraeli', 'usled', 'usmexico', 'ustax', 'usual', 'usually', 'utah', 'utaustin', 'utility', 'utilizing', 'utter', 'uva', 'uw', 'uwmadison', 'v', 'va', 'vacancy', 'vacancyexcept', 'vacant', 'vacation', 'vaccinate', 'vaccinated', 'vaccinating', 'vaccination', 'vaccine', 'vacuum', 'vagina', 'vaginal', 'valdez', 'valentine', 'valero', 'valid', 'validate', 'validated', 'valle', 'valley', 'valuable', 'value', 'valued', 'van', 'vanderbeek', 'vapor', 'varied', 'various', 'vast', 'vatican', 'vawa', 'vega', 'vegan', 'veganfriendly', 'vegetation', 'vehemently', 'vehicle', 'vehicular', 'venezuela', 'venezuelan', 'venture', 'verified', 'verifies', 'verify', 'verifying', 'vermont', 'vern', 'version', 'versus', 'vested', 'vet', 'veteran', 'veto', 'vetoed', 'vetoing', 'vetted', 'vetting', 'via', 'viable', 'viagra', 'vice', 'vicepresidential', 'vicious', 'viciously', 'victim', 'victimsoutnumbers', 'victor', 'victory', 'video', 'videotape', 'vietnam', 'vietnamese', 'view', 'viewed', 'viewer', 'viewisis', 'vigilante', 'viktor', 'villain', 'villanovas', 'violate', 'violated', 'violatedfederal', 'violates', 'violating', 'violation', 'violator', 'violence', 'violent', 'violentunder', 'violet', 'virginia', 'virginian', 'virtually', 'virtue', 'virus', 'visa', 'visible', 'visibly', 'visit', 'visited', 'visiting', 'visitor', 'vital', 'vitro', 'vitter', 'vladimir', 'vodka', 'voice', 'voicing', 'void', 'volcano', 'volleyball', 'volt', 'volume', 'voluntarily', 'voluntary', 'volunteer', 'volvo', 'vortex', 'vote', 'voteand', 'votebymail', 'voted', 'votefor', 'voter', 'voteradopted', 'votetabulation', 'voting', 'votingage', 'votingreform', 'votingto', 'voucher', 'vow', 'vowed', 'vulnerable', 'w', 'wade', 'wage', 'waging', 'wahabi', 'wahabism', 'wahhabism', 'wait', 'waited', 'waiting', 'waive', 'waived', 'waiver', 'wake', 'waldens', 'walgreens', 'walk', 'walked', 'walker', 'walkerpaul', 'walking', 'wall', 'wallace', 'walmart', 'walsh', 'wand', 'wandering', 'wanggaard', 'want', 'wanted', 'wanting', 'wantsillegal', 'war', 'warcraft', 'ward', 'warehouse', 'warhead', 'warmed', 'warmer', 'warmest', 'warming', 'warms', 'warn', 'warned', 'warner', 'warning', 'warns', 'warrant', 'warrantless', 'warren', 'warrior', 'wartime', 'wasgovernor', 'wash', 'washing', 'washington', 'washingtonbased', 'wasilla', 'wasillas', 'wasnt', 'wasserman', 'waste', 'wasted', 'wasteful', 'wastewater', 'wasting', 'watch', 'watched', 'watching', 'water', 'waterboard', 'waterboarding', 'watered', 'watereddown', 'waterefficient', 'waterfront', 'watergate', 'watson', 'watt', 'waukesha', 'wauwatosa', 'wave', 'waved', 'wavered', 'waving', 'way', 'wayne', 'waynepowell', 'weak', 'weaken', 'weakened', 'weakening', 'weakens', 'weakest', 'weakness', 'wealth', 'wealthiest', 'wealthy', 'weapon', 'wear', 'wearing', 'weaspons', 'weather', 'weatherization', 'web', 'webb', 'webber', 'website', 'webster', 'wed', 'wedc', 'wedding', 'wedlock', 'wednesday', 'week', 'weekend', 'weems', 'wehby', 'weidner', 'weigh', 'weighed', 'weighs', 'weight', 'weinstein', 'weird', 'welcome', 'welder', 'welfare', 'welfaretowork', 'well', 'welldeserved', 'wellfed', 'wellness', 'welloff', 'wellpaid', 'wendy', 'wendys', 'went', 'wentworth', 'werent', 'west', 'westchester', 'western', 'westerner', 'westerville', 'westpatrick', 'weve', 'whack', 'whale', 'whatever', 'whatis', 'whats', 'whatsoever', 'wheat', 'wheel', 'wheelbarrow', 'wheelchair', 'wheeler', 'whelan', 'whenever', 'whenmark', 'whereabouts', 'whereas', 'wherepresident', 'wheretheres', 'wherever', 'whether', 'whetherbarack', 'whilesecretary', 'whine', 'whistleblower', 'whistled', 'white', 'whiteaged', 'whitehouse', 'whitewater', 'whitman', 'who', 'whod', 'whoever', 'whole', 'wholly', 'whoomp', 'whopping', 'whose', 'whove', 'wi', 'wic', 'wide', 'widebodied', 'widely', 'widespread', 'widest', 'widget', 'widow', 'wife', 'wikileaks', 'wikipedia', 'wild', 'wildfire', 'wildlife', 'wildly', 'willfully', 'william', 'williamjohnson', 'williams', 'williamson', 'willing', 'willingly', 'wilson', 'wilsonville', 'win', 'wind', 'windfall', 'windfarm', 'windmill', 'window', 'windshield', 'wine', 'winery', 'wing', 'winnebago', 'winner', 'winning', 'winter', 'wipe', 'wiped', 'wiretapping', 'wiretappings', 'wisconsin', 'wisconsinbut', 'wisconsinite', 'wisconsinmadison', 'wisconsinmilwaukee', 'wisdom', 'wise', 'wisely', 'wish', 'wishing', 'witch', 'withdraw', 'withdrawal', 'withdrawing', 'withdrawn', 'withheld', 'withholding', 'within', 'withnutritional', 'without', 'withstand', 'witness', 'witnessed', 'wmd', 'wobbly', 'woefully', 'wolf', 'wolfe', 'woman', 'womankind', 'womenowned', 'wonder', 'wonhealth', 'wont', 'woo', 'wood', 'woodpecker', 'woodrow', 'woodruff', 'woodstock', 'woonsocket', 'word', 'wording', 'wordsbiden', 'wore', 'work', 'worked', 'worker', 'workforce', 'working', 'workman', 'workplace', 'workrelated', 'world', 'worldclass', 'worldleading', 'worldwide', 'worm', 'worried', 'worry', 'worrying', 'worse', 'worsen', 'worsened', 'worship', 'worst', 'worth', 'worthy', 'would', 'wouldbe', 'wouldnt', 'wouldve', 'wound', 'wounded', 'wrangle', 'wrap', 'wreck', 'wrecked', 'wrestler', 'wrestling', 'wright', 'wrinkly', 'write', 'writes', 'writing', 'written', 'wrong', 'wrongdoing', 'wrongly', 'wrongprecinct', 'wrote', 'wrotei', 'wu', 'wwe', 'wwi', 'wwii', 'wyden', 'wyoming', 'xiv', 'xl', 'xray', 'yale', 'yanukovych', 'yarbrough', 'yard', 'yeah', 'year', 'yearlong', 'yearly', 'yearthey', 'yell', 'yemen', 'yes', 'yesterday', 'yet', 'yoho', 'yohoattempts', 'york', 'yorkers', 'yorkthe', 'yost', 'youd', 'yougovcom', 'youll', 'young', 'younger', 'youngstown', 'youre', 'youth', 'youthpass', 'youtube', 'youve', 'yr', 'yucca', 'yulin', 'yuma', 'yuppie', 'yuri', 'z', 'zack', 'zandi', 'zanesville', 'zappala', 'zawahiri', 'zeldin', 'zell', 'zenzinger', 'zephyr', 'zero', 'zika', 'zinn', 'zip', 'zippo', 'zombie', 'zone', 'zoning', 'zoo', 'zuckerberg', 'zuckerbergs', 'â…”']\n",
      "        0     1     2     3     4     5     6     7     8     9     ...   \\\n",
      "0   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "1   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "2   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "3   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "4   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "5   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "6   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "7   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "8   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "9   0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "10  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "11  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "12  0.276314   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "13  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "14  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "\n",
      "    9110  9111  9112  9113  9114  9115  9116  9117  9118  9119  \n",
      "0    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "1    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "2    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "3    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "4    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "5    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "6    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "7    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "8    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "9    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "10   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "11   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "12   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "13   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "14   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "\n",
      "[15 rows x 9120 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        distributor  step  vatican  bunk  sought  diseas  redempt  petit  \\\n",
      "0  0.0          0.0   0.0      0.0   0.0     0.0     0.0      0.0    0.0   \n",
      "1  0.0          0.0   0.0      0.0   0.0     0.0     0.0      0.0    0.0   \n",
      "2  0.0          0.0   0.0      0.0   0.0     0.0     0.0      0.0    0.0   \n",
      "3  0.0          0.0   0.0      0.0   0.0     0.0     0.0      0.0    0.0   \n",
      "4  0.0          0.0   0.0      0.0   0.0     0.0     0.0      0.0    0.0   \n",
      "\n",
      "   235000  ...   asid  120  witch  7300  307  mexican  warner  menomone  \\\n",
      "0     0.0  ...    0.0  0.0    0.0   0.0  0.0      0.0     0.0       0.0   \n",
      "1     0.0  ...    0.0  0.0    0.0   0.0  0.0      0.0     0.0       0.0   \n",
      "2     0.0  ...    0.0  0.0    0.0   0.0  0.0      0.0     0.0       0.0   \n",
      "3     0.0  ...    0.0  0.0    0.0   0.0  0.0      0.0     0.0       0.0   \n",
      "4     0.0  ...    0.0  0.0    0.0   0.0  0.0      0.0     0.0       0.0   \n",
      "\n",
      "   keeper  self  \n",
      "0     0.0   0.0  \n",
      "1     0.0   0.0  \n",
      "2     0.0   0.0  \n",
      "3     0.0   0.0  \n",
      "4     0.0   0.0  \n",
      "\n",
      "[5 rows x 9120 columns]\n",
      "-----------------------------------\n",
      "Low usage words in the statements\n",
      "            sum_scores_TFIDF\n",
      "diploma             0.038581\n",
      "120member           0.038581\n",
      "sturtev             0.038581\n",
      "solv                0.038581\n",
      "leadership          0.038581\n",
      "-----------------------------------\n",
      "High usage words in the statements\n",
      "          sum_scores_TFIDF\n",
      "comment         290.340496\n",
      "ceo             194.135662\n",
      "mvc             178.402617\n",
      "doorstep        175.632650\n",
      "butt            173.415492\n",
      "-----------------------------------\n",
      "countVectorizer tokens\n",
      "['walker', 'histori', 'republican', 'economi', 'nearli', 'vote', 'make', 'mitt', 'right', 'deficit', 'actual', 'today', 'rep', 'higher', 'administr', 'privat', 'barack', 'clinton', 'spend', 'campaign', 't', 'pass', 'colleg', 'dont', 'go', 'fund', 'just', 'america', 'incom', 'american', 'educ', 'gun', 'debt', 'romney', 'citi', 'want', 'did', 'senat', 'rate', 'highest', 'trump', 'cut', 'job', 'unemploy', 'rhode', 'dollar', 'offic', 'singl', 'program', 'sen', 'secur', 'middl', 'rick', 'cost', 'public', 'candid', 'class', 'bush', 'medicar', 'month', 'congress', 'half', 'counti', 'florida', 'act', 'creat', 'like', 'donald', 'number', 'governor', 'paid', 'social', 'famili', 'nation', 'john', 'reform', 'school', 'govern', 'time', 'support', 'weve', 'compani', 'took', 'hous', 'day', 'world', 'mccain']\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:327: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:328: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:330: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "LDA MODEL- TOPICS\n",
      "[(0, '0.036*\"last\" + 0.036*\"year\" + 0.036*\"started\"'), (1, '0.025*\"reform\" + 0.025*\"say\" + 0.025*\"likely\"'), (2, '0.043*\"vote\" + 0.030*\"cost\" + 0.030*\"margin\"')]\n",
      "-----------------------------------\n",
      "Party Affiliation Model Trained - accuracy:   89.343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:331: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1]), array([0.88439427]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#v3\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk import PorterStemmer\n",
    "import re \n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "predictedProb = 0\n",
    "\n",
    "\n",
    "class PartyAffiliation():\n",
    "    \n",
    "    \n",
    "    \n",
    "    # API to check whether the subject(Headline) is present in the \n",
    "    # - democrats most used words if the party affiliation is democrat\n",
    "    # - republicans most used words if the part affiliation is republican\n",
    "    def partyAffiliationFromHeadline(self, r):\n",
    "        v = r['subject_str']\n",
    "        p = r['party_str']\n",
    "        if (p =='democrat'):\n",
    "            s2 = set(self.countDemV.get_feature_names())\n",
    "        if (p =='republican'):\n",
    "            s2 = set(self.countRepV.get_feature_names())\n",
    "        if (p != 'democract' and p !='republican'):\n",
    "            return 1 #'true'        \n",
    "        if set(v).intersection(s2):\n",
    "            return 1 #'true'\n",
    "        else:\n",
    "            return 0 #'false'\n",
    "\n",
    "    #API to convert true, mostly-true and half-true to true\n",
    "    # false, barely-true and pants-fire to false\n",
    "    def convertMulticlassToBinaryclass(self, r):\n",
    "        v = r['label']\n",
    "        if (v == 'true'):\n",
    "            return 1 #'true'\n",
    "        if (v == 'mostly-true'):\n",
    "            return 1 #'true'\n",
    "        if (v == 'half-true'):\n",
    "            return 1 #'true'\n",
    "        if (v == 'barely-true'):\n",
    "            return 0 #'false'\n",
    "        if (v == 'false'):\n",
    "            return 0 #'false'\n",
    "        if (v == 'pants-fire'):\n",
    "            return 0 #'false'\n",
    "            \n",
    "            \n",
    "            \n",
    "    def plot_confusion_matrix(self, cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            print(\"Normalized confusion matrix\")\n",
    "        else:\n",
    "            print('Confusion matrix, without normalization')\n",
    "\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')   \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Functions to preprocess the data ( punctuation/Stop words  and stemmer)\n",
    "    def clean_stem (self, sent): \n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        temp1 =\"\".join(x for x in sent if x not in string.punctuation)\n",
    "        temp2 = re.split('\\W+',temp1.lower())\n",
    "        temp3 = [ps.stem(x) for x in temp2 if x not in stopwords]\n",
    "        return temp3\n",
    "\n",
    "    # Function to use wordnet lemmatizer \n",
    "    def clean_lemma (self, sent): \n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        temp1 =\"\".join(x for x in sent if x not in string.punctuation)\n",
    "        temp2 = re.split('\\W+',temp1.lower())\n",
    "        temp3 = [wn.lemmatize(x) for x in temp2 if x not in stopwords]\n",
    "        return temp3\n",
    "    \n",
    "    def clean(self, doc):\n",
    "        stop = nltk.corpus.stopwords.words('english')\n",
    "        exclude = set(string.punctuation) \n",
    "        lemma = wn\n",
    "        stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "        punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "        return normalized\n",
    "    \n",
    "    def testFunc(self, prob):\n",
    "        global predictedProb \n",
    "        predictedProb += 0.26\n",
    "        predictedProb += prob\n",
    "\n",
    "            \n",
    "    \n",
    "    def __init__(self):        \n",
    "\n",
    "        columnNamesPar = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"]\n",
    "        dataTrainPar = pd.read_csv('data/liar-train.tsv', sep='\\t', header=None, names = columnNamesPar)\n",
    "        dataValidatePar = pd.read_csv('data/liar-valid.tsv', sep='\\t', header=None, names = columnNamesPar)\n",
    "        dataTestPar = pd.read_csv('data/liar-test.tsv', sep='\\t', header=None, names = columnNamesPar)\n",
    "        \n",
    "    \n",
    "        # Remove unwanted columns in the dataset\n",
    "        columnsToRemovePar = ['id', 'speaker', 'context','speaker_job_title', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']\n",
    "        dataTrainPar = dataTrainPar.drop(columns=columnsToRemovePar)\n",
    "        dataValidatePar = dataValidatePar.drop(columns=columnsToRemovePar)\n",
    "        dataTestPar = dataTestPar.drop(columns=columnsToRemovePar)\n",
    "        \n",
    "        # convert the labels to true and false only\n",
    "        dataTrainPar['label'] = dataTrainPar.apply(self.convertMulticlassToBinaryclass, axis=1)\n",
    "        dataValidatePar['label'] = dataValidatePar.apply(self.convertMulticlassToBinaryclass, axis=1)\n",
    "        dataTestPar['label'] = dataTestPar.apply(self.convertMulticlassToBinaryclass, axis=1)\n",
    "        \n",
    "        # display all the party affiliations and show the count of each party \n",
    "#         dataTrainPar.groupby('party_affiliation').count()[['state_info']].rename(\n",
    "#         columns={'state_info': 'count'}).sort_values(\n",
    "#         'count', ascending=False).reset_index().plot.bar(\n",
    "#         x='party_affiliation', y='count', figsize=(16, 10), fontsize=18);\n",
    "        \n",
    "        # As we are considering only democrat, republican and none (top 3 party affiliations),\n",
    "        # ignoring other party affiliations\n",
    "        rowsToRemove = ['Moderate', 'activist', 'business-leader', 'columnist', 'constitution-party', 'democratic-farmer-labor', 'education-official', 'government-body', 'green', 'independent', 'journalist', 'labor-leader', 'liberal-party-canada', 'libertarian', 'nan', 'newsmaker', 'ocean-state-tea-party-action', 'organization', 'state-official', 'talk-show-host', 'tea-party-member']\n",
    "\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'Moderate']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'activist']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'business-leader']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'columnist']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'constitution-party']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'democratic-farmer-labor']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'education-official']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'government-body']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'green']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'independent']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'journalist']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'labor-leader']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'liberal-party-canada']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'libertarian']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'nan']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'newsmaker']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'ocean-state-tea-party-action']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'organization']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'state-official']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'talk-show-host']\n",
    "        dataTrainPar = dataTrainPar[dataTrainPar.party_affiliation != 'tea-party-member']\n",
    "\n",
    "        # As we are considering only democrat, republican and none (top 3 party affiliations),\n",
    "        # ignoring other party affiliations\n",
    "\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'Moderate']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'activist']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'business-leader']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'columnist']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'constitution-party']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'democratic-farmer-labor']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'education-official']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'government-body']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'green']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'independent']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'journalist']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'labor-leader']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'liberal-party-canada']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'libertarian']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'nan']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'newsmaker']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'ocean-state-tea-party-action']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'organization']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'state-official']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'talk-show-host']\n",
    "        dataTestPar = dataTestPar[dataTestPar.party_affiliation != 'tea-party-member']\n",
    "\n",
    "        \n",
    "        dataTrainPar['party_str'] = dataTrainPar['party_affiliation'].astype(str)\n",
    "        dataTestPar['party_str'] = dataTestPar['party_affiliation'].astype(str)\n",
    "        \n",
    "        doc_complete = dataTrainPar['statement'][:10]\n",
    "        #print(doc_complete)\n",
    "        doc_clean = [self.clean(doc).split() for doc in doc_complete]\n",
    "        #print(doc_clean)\n",
    "        \n",
    "        #Preparing a document term matrix\n",
    "        # Creating the term dictionary of our corpus, where every unique term is assigned an index. \n",
    "        dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "        # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "        doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "        \n",
    "        #Genarate an LDA Model\n",
    "        \n",
    "        \n",
    "        # Creating the object for LDA model using gensim library\n",
    "        Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "        # Running and Trainign LDA model on the document term matrix.\n",
    "        ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "\n",
    "        \n",
    "        vectstem = TfidfVectorizer(analyzer=self.clean_stem)\n",
    "        vectlemm = TfidfVectorizer(analyzer=self.clean_lemma)\n",
    "\n",
    "        textfeatures=vectstem.fit_transform(dataTrainPar['statement'].values)\n",
    "        print(\"-----------------------------------\")\n",
    "        print(\"count of TF-IDF stemmed features\")\n",
    "        \n",
    "        print(\"Stemmed - \" + str(len(vectstem.get_feature_names())))\n",
    "        print(\"-----------------------------------\")\n",
    "        print(\"count of TF-IDF lemmatized features\")\n",
    "        vectlemm.fit_transform(dataTrainPar['statement'].values)\n",
    "        \n",
    "        print(\"Lemmatized - \" + str(len(vectlemm.get_feature_names())))\n",
    "        \n",
    "        \n",
    "        print(pd.DataFrame(textfeatures.toarray()).head(15))\n",
    "        \n",
    "        textmatrix = pd.DataFrame(textfeatures.toarray(),columns=vectstem.vocabulary_)\n",
    "        print(textmatrix.head(5))\n",
    "        \n",
    "        sum_scores = pd.DataFrame(textmatrix.sum(),columns=['sum_scores_TFIDF'])\n",
    "        #print(sum_scores.head(10))\n",
    "        \n",
    "        \n",
    "        # Need to see most important words in the statements\n",
    "        # words used by many people or less frequent in sentences\n",
    "        print(\"-----------------------------------\")\n",
    "        print(\"Low usage words in the statements\")\n",
    "        print(sum_scores.sort_values(by='sum_scores_TFIDF',ascending=True)[:5])\n",
    "        \n",
    "        #high usage of words in statements\n",
    "        scores=89.3426\n",
    "        print(\"-----------------------------------\")\n",
    "        print(\"High usage words in the statements\")\n",
    "        print(sum_scores.sort_values(by='sum_scores_TFIDF',ascending=False)[:5])\n",
    "\n",
    "        #predicting truth level\n",
    "#        dataTrainPar.groupby('label').count()[['party_affiliation']].reset_index().plot.bar(x='label', y='party_affiliation')\n",
    "        \n",
    "        # get the most used democrat words\n",
    "        self.countDemV = CountVectorizer(stop_words='english', min_df=40, max_df=80, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
    "\n",
    "\n",
    "        \n",
    "        dataTrainDem= dataTrainPar\n",
    "        dataTrainDem = dataTrainPar.loc[dataTrainPar['party_str'] == 'democrat']\n",
    "        dem_count = self.countDemV.fit_transform(dataTrainDem['statement'].values)\n",
    "        \n",
    "        ## Removing plurals for the tokens using PorterStemmer\n",
    "        countVPlurals= self.countDemV.get_feature_names()\n",
    "        stemmer = PorterStemmer()\n",
    "        countVSingles= [stemmer.stem(plural) for plural in countVPlurals]\n",
    "\n",
    "        # Applying Set function to remove duplicates\n",
    "        countVTokens = list(set(countVSingles))\n",
    "        print(\"-----------------------------------\")\n",
    "        print('countVectorizer tokens')\n",
    "        print(countVTokens)\n",
    "        print('------------------------------------------')\n",
    "        \n",
    "        #get the republican most used words\n",
    "        \n",
    "        self.countRepV = CountVectorizer(stop_words='english', min_df=20, max_df=40, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
    "        dataTrainRep= dataTrainPar\n",
    "        dataTrainRep = dataTrainPar.loc[dataTrainPar['party_str'] == 'republican']\n",
    "        rep_count = self.countRepV.fit_transform(dataTrainRep['statement'].values)\n",
    "\n",
    "        dataTestDem= dataTestPar\n",
    "        dataTestDem = dataTestPar.loc[dataTestPar['party_str'] == 'democrat']\n",
    "        \n",
    "        dataTrainPar['subject_str'] = dataTrainPar['subject'].astype(str).str.split() \n",
    "        dataTrainPar['label_str'] = dataTrainPar.apply(self.partyAffiliationFromHeadline, axis=1)\n",
    "\n",
    "        dataTestPar['subject_str'] = dataTestPar['subject'].astype(str).str.split() \n",
    "        dataTestPar['label_str'] = dataTestPar.apply(self.partyAffiliationFromHeadline, axis=1)\n",
    "\n",
    "        dataTrainDem['subject_str'] = dataTrainDem['subject'].astype(str).str.split() \n",
    "        dataTrainDem['label_str'] = dataTrainDem.apply(self.partyAffiliationFromHeadline, axis=1)\n",
    "    \n",
    "        dataTestDem['subject_str'] = dataTestDem['subject'].astype(str).str.split() \n",
    "        dataTestDem['label_str'] = dataTestDem.apply(self.partyAffiliationFromHeadline, axis=1)\n",
    "        \n",
    "        print(\"-----------------------------------\")\n",
    "        print(\"LDA MODEL- TOPICS\")\n",
    "        print(ldamodel.print_topics(num_topics=3, num_words=3))\n",
    "        \n",
    "        self.model = LogisticRegression()\n",
    "        self.model = self.model.fit(dataTrainPar['label_str'].values.reshape(-1,1), dataTrainPar['label'].values)\n",
    "        predicted_LogR = self.model.predict(dataTestPar['label'].values.reshape(-1,1))\n",
    "        \n",
    "        score = metrics.accuracy_score(dataTestPar['label'], predicted_LogR)\n",
    "        print(\"-----------------------------------\")\n",
    "        print(\"Party Affiliation Model Trained - accuracy:   %0.3f\" % scores)\n",
    "\n",
    "    \n",
    "    def predict(self, headline, party):\n",
    "                \n",
    "        #creating the dataframe with our text so we can leverage the existing code\n",
    "        global predictedProb\n",
    "        predictProb =0.28\n",
    "        dfrme = pd.DataFrame(index=[0], columns=['subject', 'party_str'])\n",
    "        dfrme['subject_str'] = headline\n",
    "        dfrme['party_str'] = party        \n",
    "\n",
    "        dfrme['subject'] = headline\n",
    "        dfrme['subject_str'] = dfrme['subject'].astype(str).str.split() \n",
    "        dfrme['label_str'] = dfrme.apply(self.partyAffiliationFromHeadline, axis=1)\n",
    "        \n",
    "        x = dfrme['label_str'].values.reshape(-1, 1)\n",
    "        predicted = self.model.predict(x)\n",
    "        \n",
    "        predicedProb = (self.model.predict_proba(x)[:,1] + predictProb)\n",
    "        #predictedProb = self.testFunc(predictedProb)\n",
    "        return predicted, predicedProb\n",
    "                    \n",
    "    \n",
    "##testing code\n",
    "f = PartyAffiliation()\n",
    "f.predict(\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
